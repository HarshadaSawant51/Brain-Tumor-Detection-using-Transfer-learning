{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yGElgqir1f5-"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "from os import listdir\n",
    "import imutils\n",
    "from sklearn.utils import shuffle\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "57MNhSzG1uNC"
   },
   "outputs": [],
   "source": [
    "#method for cropping image i.e. for getting brain area\n",
    "def crop_brain_contour(image, plot=False):\n",
    "    \n",
    "    \n",
    "    # Convert the image to grayscale, and blur it slightly\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    gray = cv2.GaussianBlur(gray, (5, 5), 0)\n",
    "\n",
    "    # Threshold the image, then perform a series of erosions +\n",
    "    # dilations to remove any small regions of noise\n",
    "    thresh = cv2.threshold(gray, 45, 255, cv2.THRESH_BINARY)[1]\n",
    "    thresh = cv2.erode(thresh, None, iterations=2)\n",
    "    thresh = cv2.dilate(thresh, None, iterations=2)\n",
    "\n",
    "    # Find contours in thresholded image, then grab the largest one\n",
    "    cnts = cv2.findContours(thresh.copy(), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    cnts = imutils.grab_contours(cnts)\n",
    "    c = max(cnts, key=cv2.contourArea)\n",
    "    \n",
    "\n",
    "    # Find the extreme points\n",
    "    extLeft = tuple(c[c[:, :, 0].argmin()][0])\n",
    "    extRight = tuple(c[c[:, :, 0].argmax()][0])\n",
    "    extTop = tuple(c[c[:, :, 1].argmin()][0])\n",
    "    extBot = tuple(c[c[:, :, 1].argmax()][0])\n",
    "    \n",
    "    # crop new image out of the original image using the four extreme points (left, right, top, bottom)\n",
    "    new_image = image[extTop[1]:extBot[1], extLeft[0]:extRight[0]]            \n",
    "\n",
    "    if plot:\n",
    "        plt.figure()\n",
    "\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.imshow(image)\n",
    "        \n",
    "        plt.tick_params(axis='both', which='both', \n",
    "                        top=False, bottom=False, left=False, right=False,\n",
    "                        labelbottom=False, labeltop=False, labelleft=False, labelright=False)\n",
    "        \n",
    "        plt.title('Original Image')\n",
    "            \n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.imshow(new_image)\n",
    "\n",
    "        plt.tick_params(axis='both', which='both', \n",
    "                        top=False, bottom=False, left=False, right=False,\n",
    "                        labelbottom=False, labeltop=False, labelleft=False, labelright=False)\n",
    "\n",
    "        plt.title('Preprocessed Image')\n",
    "        \n",
    "        plt.show()\n",
    "    \n",
    "    return new_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xMP7hmaj1uRc"
   },
   "outputs": [],
   "source": [
    "# function for loading images and labels into X and y..in X, it will store images.. \n",
    "#and in y it will store actual image label (0 or 1) for each image..if 0 no tumor,1 then tumorous image\n",
    "def load_data(dir_list, image_size):\n",
    "    \n",
    "    # load all images in a directory\n",
    "    X = []\n",
    "    y = []\n",
    "    \n",
    "    image_width, image_height = image_size\n",
    "    \n",
    "    for directory in dir_list:\n",
    "        for filename in listdir(directory):\n",
    "            # load the image\n",
    "            image = cv2.imread(directory + '/' + filename)\n",
    "            \n",
    "            # crop the brain and ignore the unnecessary rest part of the image\n",
    "            image = crop_brain_contour(image, plot=False)\n",
    "            # resize image\n",
    "            image = cv2.resize(image, dsize=(image_width, image_height), interpolation=cv2.INTER_CUBIC)\n",
    "            # normalize values\n",
    "            image = image / 255.\n",
    "            # convert image to numpy array and append it to X\n",
    "            X.append(image)\n",
    "            # append a value of 1 to the target array if the image\n",
    "            # is in the folder named 'yes', otherwise append 0.\n",
    "            if directory[-3:] == 'yes':\n",
    "                y.append([1])\n",
    "            else:\n",
    "                y.append([0])\n",
    "                \n",
    "    X = np.array(X)\n",
    "    y = np.array(y)\n",
    "    \n",
    "    # Shuffle the data\n",
    "    X, y = shuffle(X, y)\n",
    "    \n",
    "    print(f'Number of examples is: {len(X)}')\n",
    "    print(f'X shape is: {X.shape}')\n",
    "    print(f'y shape is: {y.shape}')\n",
    "    \n",
    "    return X, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cKtsSpxP11qs",
    "outputId": "90629823-87c9-4e4b-db2e-d1c07d034569"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "#code for mounting drive into the google colab..so that we can use any folder, file inside our drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U-Pzv1AK19Z3"
   },
   "outputs": [],
   "source": [
    "#path for yes and no folder\n",
    "yes = \"/content/drive/My Drive/augmented_data/yes\"\n",
    "no = \"/content/drive/My Drive/augmented_data/no\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "G9_FBjgC2AZ_",
    "outputId": "023dc784-69b7-410a-e133-377bb34f2da4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of examples is: 2065\n",
      "X shape is: (2065, 256, 256, 3)\n",
      "y shape is: (2065, 1)\n"
     ]
    }
   ],
   "source": [
    "#image size specified\n",
    "IMG_WIDTH, IMG_HEIGHT = (256, 256)\n",
    "#calling load function for loading X,y data by passing both folder paths\n",
    "X, y = load_data([yes,no], (IMG_WIDTH, IMG_HEIGHT))\n",
    "#here we can see it is showing 2065,256,256,3...means 2065 images, and each image of size is (256,256,3)\n",
    "# y shape is 2065,1 means it stores labels for 2065 images respective to X data.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2FRMn2UW1uU-"
   },
   "outputs": [],
   "source": [
    "def split_data(X, y, test_size=0.2):\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size)\n",
    "    \n",
    "    return X_train, y_train, X_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LFeMwj3f1uY6"
   },
   "outputs": [],
   "source": [
    "#data splitting\n",
    "X_train, y_train,X_test, y_test = split_data(X, y, test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YwgOZDOq1uc9",
    "outputId": "acc7331c-8c2b-4620-e6b8-2aa31507b83a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "58892288/58889256 [==============================] - 0s 0us/step\n",
      "58900480/58889256 [==============================] - 0s 0us/step\n"
     ]
    }
   ],
   "source": [
    "#loading vgg16 pretrained model\n",
    "from keras.applications.vgg16 import VGG16\n",
    "SIZE=256\n",
    "VGG_model=VGG16(input_shape=(SIZE,SIZE,3),include_top=False,weights='imagenet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eNbbkew92RgG"
   },
   "outputs": [],
   "source": [
    "#we are not not using VGG16 model for training...so we made all layers as non trainable\n",
    "for layer in VGG_model.layers:\n",
    "    layer.trainable=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ou1X-NJB2Ue9",
    "outputId": "e865d7a1-bb78-4385-c8b1-11eea38cb104"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"vgg16\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 256, 256, 3)]     0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 256, 256, 64)      1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 256, 256, 64)      36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 128, 128, 64)      0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 128, 128, 128)     73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 128, 128, 128)     147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 64, 64, 128)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 64, 64, 256)       295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 64, 64, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 64, 64, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 32, 32, 256)       0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 32, 32, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 32, 32, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 32, 32, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 16, 16, 512)       0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 16, 16, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 16, 16, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 16, 16, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 8, 8, 512)         0         \n",
      "=================================================================\n",
      "Total params: 14,714,688\n",
      "Trainable params: 0\n",
      "Non-trainable params: 14,714,688\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "VGG_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ehybWzbp3rCf",
    "outputId": "4d3c2e7b-f99a-4e29-bb2c-77dcfcf09d92"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    }
   ],
   "source": [
    "#saving vgg model into file for further use\n",
    "VGG_model.save(\"/content/drive/My Drive/VGG_model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7uvVDlry2XQA"
   },
   "outputs": [],
   "source": [
    "#extract features for training data\n",
    "feature_ex=VGG_model.predict(X_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jv08IDCmKw4i",
    "outputId": "9857b32f-d56c-4e85-cbcd-c6ad78c294c1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1445, 8, 8, 512)"
      ]
     },
     "execution_count": 14,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_ex.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2Dzi_jpfLXfW"
   },
   "outputs": [],
   "source": [
    "features=feature_ex.reshape(feature_ex.shape[0],-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IN2-fH6QL26D",
    "outputId": "b497454f-62a4-4955-bf49-6fe3d06ca73f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1445, 32768)"
      ]
     },
     "execution_count": 16,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IjAo8nYD2kR-"
   },
   "outputs": [],
   "source": [
    "#extract features for testing data\n",
    "test_feature_ex=VGG_model.predict(X_test)\n",
    "test_features=test_feature_ex.reshape(test_feature_ex.shape[0],-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fOAlL_obMJ38"
   },
   "outputs": [],
   "source": [
    "#Pass features to classifier model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "O8BQm72-2kXH",
    "outputId": "14aaf3cc-b6c4-41ce-df6e-06db6fe97cc6"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "\n",
    "#Create a svm Classifier\n",
    "clf = svm.SVC(kernel='linear') # Linear Kernel\n",
    "\n",
    "#Train the model using thetraining features extracted by VGG16 i.e.features...and the labels i.e y_train\n",
    "clf.fit(features, y_train)\n",
    "\n",
    "#Predict the response for test features\n",
    "y_pred = clf.predict(test_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8YUfCLDk21tk",
    "outputId": "1985e9cc-8632-4313-9ab6-096e573d2bdc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9870967741935484"
      ]
     },
     "execution_count": 19,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#testing accuracy\n",
    "from sklearn import metrics\n",
    "metrics.accuracy_score(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IsV_3mD425iv"
   },
   "source": [
    "# Testing accuracy is 98.70"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IMJWjWCE3UOj",
    "outputId": "c345f7de-5a68-4cfe-be4f-a37ffe94d453"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.987097\n",
      "Precision: 0.984802\n",
      "Recall: 0.990826\n",
      "F1 score: 0.987805\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import make_circles\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "\n",
    "# accuracy: (tp + tn) / (p + n)\n",
    "accuracy_svm = accuracy_score(y_test, y_pred)\n",
    "print('Accuracy: %f' % accuracy_svm)\n",
    "# precision tp / (tp + fp)\n",
    "precision_svm = precision_score(y_test, y_pred)\n",
    "print('Precision: %f' % precision_svm)\n",
    "# recall: tp / (tp + fn)\n",
    "recall_svm = recall_score(y_test, y_pred)\n",
    "print('Recall: %f' % recall_svm)\n",
    "# f1: 2 tp / (2 tp + fp + fn)\n",
    "f1_svm = f1_score(y_test, y_pred)\n",
    "print('F1 score: %f' % f1_svm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "POKs-WKfFetL"
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize = (6,6))\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=90)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "    thresh = cm.max() / 2.\n",
    "    cm = np.round(cm,2)\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, cm[i, j],\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.show()\n",
    "    plt.savefig('/content/drive/My Drive/accuracy_plot.pdf',dpi=300,bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 458
    },
    "id": "Xh-FV97F21yV",
    "outputId": "b8ff6bdc-a748-480b-8ded-0af26a80122c"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x432 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "labels = ['yes','no']\n",
    "confusion_mtx = confusion_matrix(y_test, y_pred) \n",
    "cm = plot_confusion_matrix(confusion_mtx, classes = labels, normalize=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Zdk2PMJv3iyW",
    "outputId": "de0bdd45-a0bf-4edc-a92b-0154c60a0360"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix:\n",
      "[[288   5]\n",
      " [  3 324]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Confusion matrix:\\n%s\" % metrics.confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ntkx89x-3x2K"
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "# save the model to disk\n",
    "filename = '/content/drive/My Drive/finalized_model.sav'\n",
    "pickle.dump(clf, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JN0sGKGi34sC"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A5Uw9QBo5Oy6"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WkUtC6VG5O61"
   },
   "outputs": [],
   "source": [
    "#How to see output on new input image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "95f4W2pC34wv"
   },
   "outputs": [],
   "source": [
    "#read image and preprocess it\n",
    "path=\"/content/drive/My Drive/augmented_data/no/aug_48 no._0_370.jpg\"\n",
    "img=cv2.imread(path)\n",
    "img = crop_brain_contour(img, plot=False)\n",
    "img = cv2.resize(img, dsize=(256,256), interpolation=cv2.INTER_CUBIC)\n",
    "img = img / 255.\n",
    "from keras.preprocessing import image\n",
    "test_image = image.img_to_array(img)\n",
    "test_image = np.expand_dims(test_image, axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Mp-9bmI9GqPA",
    "outputId": "b651bf55-913e-4471-970c-92fe25d61dce"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    }
   ],
   "source": [
    "vggmodel = load_model(\"/content/drive/My Drive/VGG_model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ymSCfwoj341m"
   },
   "outputs": [],
   "source": [
    "#get features for input image\n",
    "test_f=vggmodel.predict(test_image)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "E6svqCWdHK-N"
   },
   "outputs": [],
   "source": [
    "#reshape features\n",
    "test_f=test_f.reshape(test_f.shape[0],-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ulOj5T6EF_8y"
   },
   "outputs": [],
   "source": [
    "#load svm model which is generated already\n",
    "filename = '/content/drive/My Drive/finalized_model.sav' \n",
    "svm_model = pickle.load(open(filename, 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6YMVtsEI32Fu",
    "outputId": "1186a17a-5f8f-4e03-ed39-488d77c8ba12"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No tumor detected\n"
     ]
    }
   ],
   "source": [
    "# why no detected, bcz we passed image from no folder...try givving image from yes folder also within the augmented data folder\n",
    "prediction_svm=svm_model.predict(test_f)\n",
    "if(prediction_svm[0]==0):\n",
    "   print(\"No tumor detected\")\n",
    "else:\n",
    "   print(\"Tumor detected\")\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Model_Training_VGG16+SVM.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
